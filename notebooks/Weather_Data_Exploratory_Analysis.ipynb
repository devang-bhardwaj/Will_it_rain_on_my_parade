{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edff66ed",
   "metadata": {},
   "source": [
    "# Weather Data Exploratory Analysis for NASA Space Apps Challenge\n",
    "\n",
    "This notebook explores weather data for our \"Will it rain on my parade?\" project, which aims to predict rainfall and weather conditions for specific locations. This analysis will help us understand the relationships between different weather variables and precipitation patterns.\n",
    "\n",
    "## Project Objective\n",
    "Develop a predictive model that can accurately forecast weather conditions, specifically rainfall, for particular locations to help users plan outdoor activities effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7851e",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# For geospatial data handling and visualization\n",
    "import geopandas as gpd\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    HAS_CARTOPY = True\n",
    "except ImportError:\n",
    "    HAS_CARTOPY = False\n",
    "    print(\"Cartopy not installed. Some map visualizations will be disabled.\")\n",
    "\n",
    "# For statistical analysis\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# For interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Project environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path to import our custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Create helper function to ensure data directories exist\n",
    "def ensure_data_dirs():\n",
    "    \"\"\"Create data directories if they don't exist\"\"\"\n",
    "    data_dirs = [\n",
    "        os.path.join(project_root, 'data', 'raw'),\n",
    "        os.path.join(project_root, 'data', 'processed'),\n",
    "        os.path.join(project_root, 'data', 'raw', 'nasa'),\n",
    "        os.path.join(project_root, 'data', 'raw', 'noaa'),\n",
    "        os.path.join(project_root, 'data', 'raw', 'openweathermap'),\n",
    "        os.path.join(project_root, 'data', 'raw', 'scraped')\n",
    "    ]\n",
    "    \n",
    "    for directory in data_dirs:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    print(f\"Data directories created at {os.path.join(project_root, 'data')}\")\n",
    "\n",
    "ensure_data_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475bb7b",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Analysis\n",
    "\n",
    "In this section, we'll load sample weather data and analyze its structure. In a real scenario, this data would come from:\n",
    "1. NASA Earth Data (IMERG, MODIS, MERRA-2)\n",
    "2. NOAA Weather Data\n",
    "3. OpenWeatherMap API\n",
    "4. Scraped data from weather websites using AI-Agent-Scraper\n",
    "\n",
    "For this notebook, we'll create some sample data to demonstrate the analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9375d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample weather data for demonstration\n",
    "def generate_sample_weather_data(days=365, locations=3):\n",
    "    \"\"\"\n",
    "    Generate sample weather data for demonstration purposes.\n",
    "    \n",
    "    Args:\n",
    "        days: Number of days of data to generate\n",
    "        locations: Number of different locations\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with sample weather data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Sample locations (lat, lon, name)\n",
    "    location_data = [\n",
    "        (40.7128, -74.0060, \"New York\"),\n",
    "        (34.0522, -118.2437, \"Los Angeles\"),\n",
    "        (41.8781, -87.6298, \"Chicago\"),\n",
    "        (29.7604, -95.3698, \"Houston\"),\n",
    "        (39.9526, -75.1652, \"Philadelphia\")\n",
    "    ]\n",
    "    \n",
    "    # Select the first n locations\n",
    "    selected_locations = location_data[:locations]\n",
    "    \n",
    "    # Create date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Generate data\n",
    "    data = []\n",
    "    \n",
    "    for location in selected_locations:\n",
    "        lat, lon, name = location\n",
    "        \n",
    "        for date in date_range:\n",
    "            # Base temperature with seasonal pattern (higher in summer, lower in winter)\n",
    "            # Northern hemisphere seasonal pattern\n",
    "            day_of_year = date.dayofyear\n",
    "            seasonal_factor = np.sin(2 * np.pi * (day_of_year - 15) / 365)  # Peak in mid-July\n",
    "            \n",
    "            # Temperature depends on location and season\n",
    "            base_temp = 15 + 10 * seasonal_factor  # Base temperature with seasonal variation\n",
    "            \n",
    "            # For southern locations, make it warmer\n",
    "            lat_factor = (90 - abs(lat)) / 30  # Higher for locations closer to equator\n",
    "            temp_with_lat = base_temp + lat_factor * 5\n",
    "            \n",
    "            # Add some random variation\n",
    "            temperature = temp_with_lat + np.random.normal(0, 2)\n",
    "            \n",
    "            # Humidity: inversely related to temperature but with variation\n",
    "            humidity = 70 - (temperature - 15) + np.random.normal(0, 10)\n",
    "            humidity = max(min(humidity, 100), 20)  # Keep between 20% and 100%\n",
    "            \n",
    "            # Pressure: normal atmospheric with random variation\n",
    "            pressure = 1013 + np.random.normal(0, 5)\n",
    "            \n",
    "            # Wind speed: random with some correlation to pressure changes\n",
    "            wind_speed = max(0, np.random.gamma(2, 2) + (pressure - 1013) / 10)\n",
    "            \n",
    "            # Wind direction: random\n",
    "            wind_direction = np.random.uniform(0, 360)\n",
    "            \n",
    "            # Cloud cover: correlated with humidity\n",
    "            cloud_cover = humidity * 0.8 + np.random.normal(0, 10)\n",
    "            cloud_cover = max(min(cloud_cover, 100), 0)  # Keep between 0% and 100%\n",
    "            \n",
    "            # Dew point: based on temperature and humidity\n",
    "            dew_point = temperature - ((100 - humidity) / 5) + np.random.normal(0, 1)\n",
    "            \n",
    "            # Precipitation: more likely with higher humidity and cloud cover\n",
    "            precip_probability = (humidity + cloud_cover) / 200  # Scale to 0-1\n",
    "            precip_intensity = np.random.exponential(1) if np.random.random() < precip_probability else 0\n",
    "            \n",
    "            # More precipitation in spring/fall for mid latitudes\n",
    "            season_precip_factor = np.abs(seasonal_factor) * (1 - abs(lat) / 90)\n",
    "            precipitation = precip_intensity * season_precip_factor if precip_intensity > 0 else 0\n",
    "            \n",
    "            # UV Index: higher in summer, lower in winter, affected by cloud cover\n",
    "            max_uv = 10 * (0.5 + 0.5 * seasonal_factor) * (1 - cloud_cover / 200)\n",
    "            uv_index = max(0, max_uv + np.random.normal(0, 1))\n",
    "            \n",
    "            # Visibility: inversely related to precipitation and humidity\n",
    "            visibility = 10 - (precipitation * 2) - (humidity - 50) / 10 + np.random.normal(0, 1)\n",
    "            visibility = max(min(visibility, 10), 0.1)  # Keep between 0.1 and 10 km\n",
    "            \n",
    "            data.append({\n",
    "                'date': date,\n",
    "                'location': name,\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'temperature': round(temperature, 1),\n",
    "                'humidity': round(humidity, 1),\n",
    "                'pressure': round(pressure, 1),\n",
    "                'wind_speed': round(wind_speed, 1),\n",
    "                'wind_direction': round(wind_direction, 1),\n",
    "                'cloud_cover': round(cloud_cover, 1),\n",
    "                'dew_point': round(dew_point, 1),\n",
    "                'precipitation': round(precipitation, 2),\n",
    "                'uv_index': round(uv_index, 1),\n",
    "                'visibility': round(visibility, 1)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate sample data for one year across multiple locations\n",
    "weather_data = generate_sample_weather_data(days=365, locations=5)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Sample weather data shape: {weather_data.shape}\")\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loading data from different sources and combining them\n",
    "def simulate_data_sources():\n",
    "    \"\"\"Simulate loading data from multiple sources with slightly different structures\"\"\"\n",
    "    \n",
    "    # Base data is our weather_data from above\n",
    "    base_data = weather_data.copy()\n",
    "    \n",
    "    # Subset 1: \"NASA\" data - focus on cloud cover, temperature, and precipitation\n",
    "    nasa_data = base_data[['date', 'location', 'latitude', 'longitude', \n",
    "                           'temperature', 'cloud_cover', 'precipitation']].copy()\n",
    "    nasa_data.rename(columns={'precipitation': 'precipitation_mm',\n",
    "                             'temperature': 'surface_temp_celsius'}, inplace=True)\n",
    "    nasa_data['source'] = 'NASA'\n",
    "    \n",
    "    # Subset 2: \"NOAA\" data - focus on full weather metrics\n",
    "    noaa_data = base_data[['date', 'location', 'latitude', 'longitude', \n",
    "                          'temperature', 'humidity', 'pressure', 'wind_speed',\n",
    "                          'wind_direction', 'precipitation']].copy()\n",
    "    noaa_data.rename(columns={'precipitation': 'rainfall_mm'}, inplace=True)\n",
    "    noaa_data['source'] = 'NOAA'\n",
    "    \n",
    "    # Subset 3: \"OpenWeatherMap\" data - comprehensive metrics\n",
    "    owm_data = base_data.copy()\n",
    "    owm_data.rename(columns={'precipitation': 'rain_1h'}, inplace=True)\n",
    "    owm_data['source'] = 'OpenWeatherMap'\n",
    "    \n",
    "    # Save sample datasets to the data/raw directory for future use\n",
    "    data_dir = os.path.join(project_root, 'data', 'raw')\n",
    "    \n",
    "    nasa_data.to_csv(os.path.join(data_dir, 'nasa', 'nasa_sample_data.csv'), index=False)\n",
    "    noaa_data.to_csv(os.path.join(data_dir, 'noaa', 'noaa_sample_data.csv'), index=False)\n",
    "    owm_data.to_csv(os.path.join(data_dir, 'openweathermap', 'owm_sample_data.csv'), index=False)\n",
    "    \n",
    "    print(\"Sample data files saved to data/raw directory\")\n",
    "    return nasa_data, noaa_data, owm_data\n",
    "\n",
    "# Generate and save data from simulated sources\n",
    "nasa_data, noaa_data, owm_data = simulate_data_sources()\n",
    "\n",
    "# Display sample from each source\n",
    "print(\"\\nSample NASA data:\")\n",
    "print(nasa_data.head(3))\n",
    "\n",
    "print(\"\\nSample NOAA data:\")\n",
    "print(noaa_data.head(3))\n",
    "\n",
    "print(\"\\nSample OpenWeatherMap data:\")\n",
    "print(owm_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e1a49",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Now let's perform exploratory data analysis on our weather data to understand patterns, distributions, and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of our weather data\n",
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data types:\")\n",
    "print(weather_data.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(weather_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key weather variables\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Temperature distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.histplot(weather_data['temperature'], kde=True)\n",
    "plt.title('Temperature Distribution')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "\n",
    "# Humidity distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.histplot(weather_data['humidity'], kde=True)\n",
    "plt.title('Humidity Distribution')\n",
    "plt.xlabel('Humidity (%)')\n",
    "\n",
    "# Precipitation distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "# Use log scale due to many zero values\n",
    "sns.histplot(weather_data['precipitation'].clip(lower=0.01), kde=True, log_scale=True)\n",
    "plt.title('Precipitation Distribution (log scale)')\n",
    "plt.xlabel('Precipitation (mm)')\n",
    "\n",
    "# Wind speed distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.histplot(weather_data['wind_speed'], kde=True)\n",
    "plt.title('Wind Speed Distribution')\n",
    "plt.xlabel('Wind Speed (m/s)')\n",
    "\n",
    "# Cloud cover distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(weather_data['cloud_cover'], kde=True)\n",
    "plt.title('Cloud Cover Distribution')\n",
    "plt.xlabel('Cloud Cover (%)')\n",
    "\n",
    "# Pressure distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.histplot(weather_data['pressure'], kde=True)\n",
    "plt.title('Pressure Distribution')\n",
    "plt.xlabel('Pressure (hPa)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between weather variables\n",
    "correlation_matrix = weather_data.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Weather Variables', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on correlation with precipitation\n",
    "precip_corr = correlation_matrix['precipitation'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with Precipitation:\")\n",
    "print(precip_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis of temperature and precipitation\n",
    "# Group by date and location, then calculate mean values\n",
    "daily_avg = weather_data.groupby(['date', 'location']).agg({\n",
    "    'temperature': 'mean',\n",
    "    'precipitation': 'sum',\n",
    "    'humidity': 'mean',\n",
    "    'cloud_cover': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create a pivot table for easier visualization\n",
    "pivot_temp = daily_avg.pivot(index='date', columns='location', values='temperature')\n",
    "pivot_precip = daily_avg.pivot(index='date', columns='location', values='precipitation')\n",
    "\n",
    "# Plot temperature time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_temp.plot(linewidth=1, alpha=0.8)\n",
    "plt.title('Average Daily Temperature by Location', fontsize=16)\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(title='Location')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot precipitation time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_precip.plot(linewidth=1, alpha=0.8)\n",
    "plt.title('Daily Precipitation by Location', fontsize=16)\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(title='Location')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c249d",
   "metadata": {},
   "source": [
    "## Exploring Rain Patterns and Conditions\n",
    "\n",
    "Now that we've examined the basic relationships between weather variables, let's focus specifically on analyzing rain patterns and conditions to better understand when precipitation occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e36a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary rain occurrence variable\n",
    "weather_data['rain_occurred'] = (weather_data['precipitation'] > 0).astype(int)\n",
    "\n",
    "# Analyze conditions when rain occurs\n",
    "rain_conditions = weather_data.groupby('rain_occurred').agg({\n",
    "    'temperature': ['mean', 'std'],\n",
    "    'humidity': ['mean', 'std'],\n",
    "    'cloud_cover': ['mean', 'std'],\n",
    "    'wind_speed': ['mean', 'std'],\n",
    "    'pressure': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(\"Weather conditions when rain occurs vs doesn't occur:\")\n",
    "print(rain_conditions)\n",
    "\n",
    "# Visualize distributions for rainy vs non-rainy days\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Temperature distribution\n",
    "sns.kdeplot(\n",
    "    data=weather_data, x=\"temperature\", hue=\"rain_occurred\",\n",
    "    palette={0: \"skyblue\", 1: \"darkblue\"}, \n",
    "    fill=True, alpha=0.5, ax=axes[0, 0]\n",
    ")\n",
    "axes[0, 0].set_title(\"Temperature Distribution: Rain vs No Rain\", fontsize=14)\n",
    "axes[0, 0].set_xlabel(\"Temperature (°C)\")\n",
    "axes[0, 0].legend(labels=['No Rain', 'Rain'])\n",
    "\n",
    "# Humidity distribution\n",
    "sns.kdeplot(\n",
    "    data=weather_data, x=\"humidity\", hue=\"rain_occurred\",\n",
    "    palette={0: \"palegreen\", 1: \"darkgreen\"}, \n",
    "    fill=True, alpha=0.5, ax=axes[0, 1]\n",
    ")\n",
    "axes[0, 1].set_title(\"Humidity Distribution: Rain vs No Rain\", fontsize=14)\n",
    "axes[0, 1].set_xlabel(\"Humidity (%)\")\n",
    "axes[0, 1].legend(labels=['No Rain', 'Rain'])\n",
    "\n",
    "# Cloud cover distribution\n",
    "sns.kdeplot(\n",
    "    data=weather_data, x=\"cloud_cover\", hue=\"rain_occurred\",\n",
    "    palette={0: \"lightsalmon\", 1: \"darkred\"}, \n",
    "    fill=True, alpha=0.5, ax=axes[1, 0]\n",
    ")\n",
    "axes[1, 0].set_title(\"Cloud Cover Distribution: Rain vs No Rain\", fontsize=14)\n",
    "axes[1, 0].set_xlabel(\"Cloud Cover (%)\")\n",
    "axes[1, 0].legend(labels=['No Rain', 'Rain'])\n",
    "\n",
    "# Pressure distribution\n",
    "sns.kdeplot(\n",
    "    data=weather_data, x=\"pressure\", hue=\"rain_occurred\",\n",
    "    palette={0: \"plum\", 1: \"purple\"}, \n",
    "    fill=True, alpha=0.5, ax=axes[1, 1]\n",
    ")\n",
    "axes[1, 1].set_title(\"Pressure Distribution: Rain vs No Rain\", fontsize=14)\n",
    "axes[1, 1].set_xlabel(\"Pressure (hPa)\")\n",
    "axes[1, 1].legend(labels=['No Rain', 'Rain'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181e85a",
   "metadata": {},
   "source": [
    "## Feature Engineering for Rainfall Prediction\n",
    "\n",
    "To improve our predictive models, let's engineer additional features that might help identify rainfall patterns. We'll create features like:\n",
    "\n",
    "1. Rolling statistics for temperature and humidity\n",
    "2. Weather change indicators (pressure deltas, temperature swings)\n",
    "3. Seasonal indicators\n",
    "4. Time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame to avoid modifying the original\n",
    "df_features = weather_data.copy()\n",
    "\n",
    "# Convert date to datetime if not already\n",
    "df_features['date'] = pd.to_datetime(df_features['date'])\n",
    "\n",
    "# Extract time-based features\n",
    "df_features['year'] = df_features['date'].dt.year\n",
    "df_features['month'] = df_features['date'].dt.month\n",
    "df_features['day'] = df_features['date'].dt.day\n",
    "df_features['dayofweek'] = df_features['date'].dt.dayofweek\n",
    "df_features['season'] = pd.cut(\n",
    "    df_features['date'].dt.month, \n",
    "    bins=[0, 3, 6, 9, 12], \n",
    "    labels=['Winter', 'Spring', 'Summer', 'Fall'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Group by location to calculate rolling features\n",
    "locations = df_features['location'].unique()\n",
    "feature_dfs = []\n",
    "\n",
    "for location in locations:\n",
    "    # Filter data for this location\n",
    "    loc_data = df_features[df_features['location'] == location].sort_values('date')\n",
    "    \n",
    "    # Calculate rolling statistics (3-day windows)\n",
    "    loc_data['temp_rolling_mean_3d'] = loc_data['temperature'].rolling(window=3, min_periods=1).mean()\n",
    "    loc_data['temp_rolling_std_3d'] = loc_data['temperature'].rolling(window=3, min_periods=1).std()\n",
    "    loc_data['humidity_rolling_mean_3d'] = loc_data['humidity'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Calculate pressure and temperature deltas (change from previous day)\n",
    "    loc_data['pressure_delta'] = loc_data['pressure'].diff().fillna(0)\n",
    "    loc_data['temp_delta'] = loc_data['temperature'].diff().fillna(0)\n",
    "    \n",
    "    # Add to the list of dataframes\n",
    "    feature_dfs.append(loc_data)\n",
    "\n",
    "# Combine all locations back together\n",
    "df_features = pd.concat(feature_dfs)\n",
    "\n",
    "# Create interaction features\n",
    "df_features['temp_humidity_interaction'] = df_features['temperature'] * df_features['humidity']\n",
    "df_features['wind_temp_interaction'] = df_features['wind_speed'] * df_features['temperature']\n",
    "\n",
    "# Show the first few rows with new features\n",
    "display(df_features.head())\n",
    "\n",
    "# Check correlation of new features with precipitation\n",
    "correlation_with_precip = df_features.drop(columns=['date', 'location', 'rain_occurred', 'season']).corrwith(df_features['precipitation'])\n",
    "print(\"\\nCorrelation of features with precipitation:\")\n",
    "print(correlation_with_precip.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b37224",
   "metadata": {},
   "source": [
    "## Geographical Analysis of Rainfall Patterns\n",
    "\n",
    "Weather patterns can vary significantly based on location. Let's analyze how precipitation patterns differ across our sampled locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precipitation statistics by location\n",
    "location_rain_stats = weather_data.groupby('location').agg({\n",
    "    'precipitation': ['mean', 'median', 'max', 'std', lambda x: (x > 0).mean() * 100],\n",
    "    'temperature': ['mean', 'std'],\n",
    "    'humidity': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename the custom lambda function column\n",
    "location_rain_stats.columns = location_rain_stats.columns.map('_'.join)\n",
    "location_rain_stats = location_rain_stats.rename(columns={'precipitation_<lambda>': 'precipitation_days_percent'})\n",
    "\n",
    "# Sort by average precipitation\n",
    "location_rain_stats = location_rain_stats.sort_values('precipitation_mean', ascending=False)\n",
    "\n",
    "# Display statistics by location\n",
    "print(\"Precipitation Statistics by Location:\")\n",
    "display(location_rain_stats)\n",
    "\n",
    "# Visualize average precipitation by location\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=location_rain_stats.index, \n",
    "    y=location_rain_stats['precipitation_mean'],\n",
    "    palette=\"Blues_d\"\n",
    ")\n",
    "plt.title(\"Average Daily Precipitation by Location\", fontsize=16)\n",
    "plt.ylabel(\"Average Precipitation (mm)\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize percentage of rainy days by location\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=location_rain_stats.index,\n",
    "    y=location_rain_stats['precipitation_days_percent'],\n",
    "    palette=\"Greens_d\"\n",
    ")\n",
    "plt.title(\"Percentage of Rainy Days by Location\", fontsize=16)\n",
    "plt.ylabel(\"Rainy Days (%)\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate monthly precipitation by location\n",
    "monthly_precip = weather_data.copy()\n",
    "monthly_precip['month'] = pd.to_datetime(monthly_precip['date']).dt.month\n",
    "monthly_location_rain = monthly_precip.groupby(['location', 'month'])['precipitation'].mean().reset_index()\n",
    "\n",
    "# Create a pivot table for visualization\n",
    "monthly_location_pivot = monthly_location_rain.pivot(index='month', columns='location', values='precipitation')\n",
    "\n",
    "# Plot monthly precipitation patterns by location\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    monthly_location_pivot, \n",
    "    cmap='YlGnBu', \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    linewidths=.5\n",
    ")\n",
    "plt.title(\"Monthly Average Precipitation by Location\", fontsize=16)\n",
    "plt.ylabel(\"Month\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea87868",
   "metadata": {},
   "source": [
    "## Preparing Data for Machine Learning Models\n",
    "\n",
    "Now that we've done extensive exploratory analysis and feature engineering, let's prepare our data for machine learning models. We'll focus on two prediction tasks:\n",
    "\n",
    "1. **Rain Occurrence Classification**: Predicting whether it will rain or not (binary classification)\n",
    "2. **Rainfall Amount Regression**: Predicting how much rain will fall (regression)\n",
    "\n",
    "Let's prepare the data for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6428ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Use the engineered features DataFrame\n",
    "ml_data = df_features.copy()\n",
    "\n",
    "# Drop rows with missing values or handle them appropriately\n",
    "ml_data = ml_data.dropna()\n",
    "\n",
    "# Define features and target variables\n",
    "# For classification: predict whether it will rain\n",
    "X = ml_data.drop(columns=['precipitation', 'rain_occurred', 'date'])\n",
    "y_class = ml_data['rain_occurred']\n",
    "\n",
    "# For regression: predict amount of rain (only on days when it rained)\n",
    "rain_data = ml_data[ml_data['precipitation'] > 0].copy()\n",
    "X_reg = rain_data.drop(columns=['precipitation', 'rain_occurred', 'date'])\n",
    "y_reg = rain_data['precipitation']\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Classification dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Regression dataset: {X_reg.shape[0]} samples, {X_reg.shape[1]} features\")\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_features = ['location', 'season']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create preprocessor for both categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTrain/Test Split:\")\n",
    "print(f\"Classification - Training: {X_train.shape[0]} samples, Testing: {X_test.shape[0]} samples\")\n",
    "print(f\"Regression - Training: {X_train_reg.shape[0]} samples, Testing: {X_test_reg.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce616728",
   "metadata": {},
   "source": [
    "## Building and Evaluating Machine Learning Models\n",
    "\n",
    "Now let's build our two machine learning models:\n",
    "1. A random forest classifier to predict whether it will rain\n",
    "2. A random forest regressor to predict the amount of rainfall\n",
    "\n",
    "We'll evaluate both models and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rain Occurrence Classification Model\n",
    "# Create a pipeline with preprocessing and classifier\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training rain occurrence classifier...\")\n",
    "classification_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class = classification_pipeline.predict(X_test)\n",
    "y_pred_proba = classification_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"\\nClassification Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "\n",
    "# 2. Rainfall Amount Regression Model\n",
    "# Create a pipeline with preprocessing and regressor\n",
    "regression_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining rainfall amount regressor...\")\n",
    "regression_pipeline.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = regression_pipeline.predict(X_test_reg)\n",
    "\n",
    "# Evaluate the regressor\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"\\nRegression Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importances for both models\n",
    "def plot_feature_importance(model, preprocessor, feature_names, title):\n",
    "    # Extract feature names after preprocessing (including one-hot encoded categories)\n",
    "    if hasattr(preprocessor, 'transformers_'):\n",
    "        # For the ColumnTransformer in a Pipeline\n",
    "        categorical_features_idx = preprocessor.transformers_[1][2]  # Index of categorical features\n",
    "        categorical_features = [feature_names[i] for i in categorical_features_idx]\n",
    "        onehot = preprocessor.transformers_[1][1]  # OneHotEncoder\n",
    "        if hasattr(onehot, 'categories_'):\n",
    "            onehot_feature_names = []\n",
    "            for i, cats in enumerate(onehot.categories_):\n",
    "                for cat in cats:\n",
    "                    onehot_feature_names.append(f\"{categorical_features[i]}_{cat}\")\n",
    "            \n",
    "            # Get numerical feature names\n",
    "            numerical_features_idx = preprocessor.transformers_[0][2]  # Index of numerical features\n",
    "            numerical_features = [feature_names[i] for i in numerical_features_idx]\n",
    "            \n",
    "            # Combine all feature names\n",
    "            all_feature_names = numerical_features + onehot_feature_names\n",
    "        else:\n",
    "            # Fallback if OneHotEncoder has no categories_ attribute\n",
    "            all_feature_names = feature_names\n",
    "    else:\n",
    "        # Fallback to original feature names\n",
    "        all_feature_names = feature_names\n",
    "    \n",
    "    # Get feature importances\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Create DataFrame for easier sorting and plotting\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': all_feature_names[:len(importances)],\n",
    "            'Importance': importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20))\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(\"Model does not have feature_importances_ attribute\")\n",
    "        return None\n",
    "\n",
    "# Extract the classifier and regressor from the pipelines\n",
    "classifier = classification_pipeline.named_steps['classifier']\n",
    "regressor = regression_pipeline.named_steps['regressor']\n",
    "preprocessor_clf = classification_pipeline.named_steps['preprocessor']\n",
    "preprocessor_reg = regression_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Plot feature importances\n",
    "print(\"Top Features for Rain Occurrence Prediction:\")\n",
    "clf_importances = plot_feature_importance(\n",
    "    classifier, preprocessor_clf, \n",
    "    list(X.columns), \n",
    "    \"Feature Importance for Rain Occurrence Prediction\"\n",
    ")\n",
    "display(clf_importances.head(10))\n",
    "\n",
    "print(\"\\nTop Features for Rainfall Amount Prediction:\")\n",
    "reg_importances = plot_feature_importance(\n",
    "    regressor, preprocessor_reg, \n",
    "    list(X_reg.columns), \n",
    "    \"Feature Importance for Rainfall Amount Prediction\"\n",
    ")\n",
    "display(reg_importances.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280119c8",
   "metadata": {},
   "source": [
    "## Testing Prediction on New Data\n",
    "\n",
    "Let's demonstrate how to use our trained models to make predictions for specific locations and dates. This is the functionality that will be core to our \"Will it rain on my parade?\" application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample new data for prediction\n",
    "# This would typically come from a weather forecast API\n",
    "# but we'll create some synthetic examples\n",
    "\n",
    "def prepare_prediction_data(location, date_str, forecast_data):\n",
    "    \"\"\"\n",
    "    Prepare a single data point for prediction based on forecast data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    location : str\n",
    "        The name of the location\n",
    "    date_str : str\n",
    "        Date string in 'YYYY-MM-DD' format\n",
    "    forecast_data : dict\n",
    "        Dictionary containing forecast weather variables\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A dataframe with a single row prepared for prediction\n",
    "    \"\"\"\n",
    "    # Create a dataframe with the basic forecast data\n",
    "    df = pd.DataFrame([forecast_data])\n",
    "    \n",
    "    # Add location and date\n",
    "    df['location'] = location\n",
    "    df['date'] = pd.to_datetime(date_str)\n",
    "    \n",
    "    # Add the engineered features we used during training\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    \n",
    "    # Create season\n",
    "    df['season'] = pd.cut(\n",
    "        df['date'].dt.month, \n",
    "        bins=[0, 3, 6, 9, 12], \n",
    "        labels=['Winter', 'Spring', 'Summer', 'Fall'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Since we don't have historical data for this new point,\n",
    "    # use the provided forecast data for the rolling features\n",
    "    df['temp_rolling_mean_3d'] = df['temperature']\n",
    "    df['temp_rolling_std_3d'] = 0\n",
    "    df['humidity_rolling_mean_3d'] = df['humidity']\n",
    "    df['pressure_delta'] = 0\n",
    "    df['temp_delta'] = 0\n",
    "    \n",
    "    # Create interaction features\n",
    "    df['temp_humidity_interaction'] = df['temperature'] * df['humidity']\n",
    "    df['wind_temp_interaction'] = df['wind_speed'] * df['temperature']\n",
    "    \n",
    "    # Drop columns not needed for prediction\n",
    "    if 'precipitation' in df.columns:\n",
    "        df = df.drop(columns=['precipitation'])\n",
    "    if 'rain_occurred' in df.columns:\n",
    "        df = df.drop(columns=['rain_occurred'])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Create a few sample forecast data points\n",
    "sample_forecasts = [\n",
    "    {\n",
    "        'location': 'New York',\n",
    "        'date': '2023-10-10',\n",
    "        'forecast': {\n",
    "            'temperature': 18.5,\n",
    "            'humidity': 72,\n",
    "            'cloud_cover': 65,\n",
    "            'wind_speed': 12,\n",
    "            'pressure': 1013.2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'location': 'San Francisco',\n",
    "        'date': '2023-10-10',\n",
    "        'forecast': {\n",
    "            'temperature': 16.8,\n",
    "            'humidity': 68,\n",
    "            'cloud_cover': 30,\n",
    "            'wind_speed': 8,\n",
    "            'pressure': 1016.5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'location': 'Miami',\n",
    "        'date': '2023-10-10',\n",
    "        'forecast': {\n",
    "            'temperature': 28.2,\n",
    "            'humidity': 85,\n",
    "            'cloud_cover': 75,\n",
    "            'wind_speed': 15,\n",
    "            'pressure': 1009.8\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make predictions for each sample\n",
    "print(\"Predictions for Sample Forecast Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sample in sample_forecasts:\n",
    "    location = sample['location']\n",
    "    date = sample['date']\n",
    "    forecast = sample['forecast']\n",
    "    \n",
    "    # Prepare the data for prediction\n",
    "    pred_data = prepare_prediction_data(location, date, forecast)\n",
    "    \n",
    "    # Make predictions\n",
    "    rain_prob = classification_pipeline.predict_proba(pred_data)[0, 1]\n",
    "    will_rain = classification_pipeline.predict(pred_data)[0]\n",
    "    \n",
    "    # If it's predicted to rain, estimate the amount\n",
    "    rain_amount = 0\n",
    "    if will_rain == 1:\n",
    "        rain_amount = regression_pipeline.predict(pred_data)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Location: {location}, Date: {date}\")\n",
    "    print(f\"Forecast: Temp={forecast['temperature']}°C, Humidity={forecast['humidity']}%, Pressure={forecast['pressure']}hPa\")\n",
    "    print(f\"Prediction: {rain_prob*100:.1f}% chance of rain\")\n",
    "    \n",
    "    if will_rain == 1:\n",
    "        print(f\"Expected rainfall: {rain_amount:.2f} mm\")\n",
    "        print(f\"Recommendation: Consider bringing an umbrella!\")\n",
    "    else:\n",
    "        print(\"No rain expected\")\n",
    "        print(\"Recommendation: Enjoy your outdoor activities!\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58e973",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this exploratory analysis notebook, we've:\n",
    "\n",
    "1. Generated and analyzed sample weather data to understand the relationships between weather variables\n",
    "2. Performed feature engineering to create useful predictors for rainfall\n",
    "3. Analyzed geographical and seasonal patterns in precipitation\n",
    "4. Built and evaluated machine learning models for predicting both rain occurrence and amount\n",
    "5. Demonstrated how to use the models for real-world predictions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Integrate with Real Data Sources**: \n",
    "   - Replace our simulated data with real weather data from NASA, NOAA, and OpenWeatherMap\n",
    "   - Configure AI-Agent-Scraper for efficient web scraping of weather data\n",
    "\n",
    "2. **Enhance the Models**:\n",
    "   - Experiment with different machine learning algorithms\n",
    "   - Perform hyperparameter tuning to improve model performance\n",
    "   - Consider using deep learning for time series forecasting\n",
    "\n",
    "3. **Feature Development**:\n",
    "   - Add more sophisticated features like atmospheric indices and radar data\n",
    "   - Incorporate satellite imagery for cloud pattern recognition\n",
    "\n",
    "4. **Application Development**:\n",
    "   - Build a user-friendly API for \"Will it rain on my parade?\" predictions\n",
    "   - Develop a front-end interface for users to input their event details\n",
    "   - Create visualization components for the prediction results\n",
    "\n",
    "5. **Validation and Testing**:\n",
    "   - Validate predictions against actual weather outcomes\n",
    "   - Implement a feedback mechanism to improve model accuracy over time\n",
    "   - Test the system with various use cases and locations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
